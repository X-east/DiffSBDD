#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ£€æŸ¥ç‚¹å…¨é¢åˆ†æè„šæœ¬
ç”¨äºæ·±å…¥åˆ†æDiffSBDDæ¨¡å‹æ£€æŸ¥ç‚¹çš„ç»“æ„ã€å‚æ•°å’Œé…ç½®
"""

import torch
import json
import numpy as np
from pathlib import Path
from collections import OrderedDict
from datetime import datetime
import sys
import os

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))


class CheckpointAnalyzer:
    """æ£€æŸ¥ç‚¹åˆ†æå™¨"""
    
    def __init__(self, checkpoint_path):
        """åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            checkpoint_path: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
        """
        self.checkpoint_path = Path(checkpoint_path)
        self.checkpoint = None
        self.analysis_results = {}
        
        if not self.checkpoint_path.exists():
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        
        print(f"[ä¿¡æ¯] åŠ è½½æ£€æŸ¥ç‚¹: {self.checkpoint_path}")
        self.load_checkpoint()
        
    def load_checkpoint(self):
        """åŠ è½½æ£€æŸ¥ç‚¹æ–‡ä»¶"""
        try:
            # åœ¨CPUä¸ŠåŠ è½½ï¼Œé¿å…GPUå†…å­˜é—®é¢˜
            # PyTorch 2.6+ éœ€è¦ weights_only=False æ¥åŠ è½½åŒ…å«è‡ªå®šä¹‰ç±»çš„æ£€æŸ¥ç‚¹
            self.checkpoint = torch.load(
                self.checkpoint_path,
                map_location=torch.device('cpu'),
                weights_only=False
            )
            print(f"[æˆåŠŸ] æ£€æŸ¥ç‚¹åŠ è½½å®Œæˆ")
        except Exception as e:
            raise RuntimeError(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
    
    def get_file_info(self):
        """è·å–æ–‡ä»¶åŸºæœ¬ä¿¡æ¯"""
        file_size = self.checkpoint_path.stat().st_size
        file_size_mb = file_size / (1024 * 1024)
        
        info = {
            "æ–‡ä»¶å": self.checkpoint_path.name,
            "æ–‡ä»¶è·¯å¾„": str(self.checkpoint_path.absolute()),
            "æ–‡ä»¶å¤§å°": f"{file_size_mb:.2f} MB ({file_size:,} bytes)",
            "ä¿®æ”¹æ—¶é—´": datetime.fromtimestamp(
                self.checkpoint_path.stat().st_mtime
            ).strftime('%Y-%m-%d %H:%M:%S')
        }
        
        self.analysis_results['file_info'] = info
        return info
    
    def analyze_checkpoint_structure(self):
        """åˆ†ææ£€æŸ¥ç‚¹çš„æ•´ä½“ç»“æ„"""
        structure = {
            "é¡¶å±‚é”®": list(self.checkpoint.keys()),
            "å„é”®çš„ç±»å‹": {k: type(v).__name__ for k, v in self.checkpoint.items()}
        }
        
        # æ£€æŸ¥å¸¸è§çš„é”®
        if 'epoch' in self.checkpoint:
            structure['è®­ç»ƒè½®æ•°'] = self.checkpoint['epoch']
        if 'global_step' in self.checkpoint:
            structure['å…¨å±€æ­¥æ•°'] = self.checkpoint['global_step']
        
        self.analysis_results['structure'] = structure
        return structure
    
    def analyze_hyperparameters(self):
        """åˆ†æè¶…å‚æ•°"""
        if 'hyper_parameters' not in self.checkpoint:
            return {"é”™è¯¯": "æœªæ‰¾åˆ°è¶…å‚æ•°"}
        
        hparams = self.checkpoint['hyper_parameters']
        
        # æå–å…³é”®è¶…å‚æ•°
        key_params = {}
        
        # è®­ç»ƒå‚æ•°
        training_params = {
            'batch_size': hparams.get('batch_size'),
            'lr': hparams.get('lr'),
            'num_workers': hparams.get('num_workers'),
            'augment_noise': hparams.get('augment_noise'),
            'augment_rotation': hparams.get('augment_rotation'),
            'clip_grad': hparams.get('clip_grad'),
        }
        key_params['è®­ç»ƒå‚æ•°'] = training_params
        
        # EGNNå‚æ•°
        if 'egnn_params' in hparams:
            egnn = hparams['egnn_params']
            egnn_params = {
                'n_layers': egnn.n_layers if hasattr(egnn, 'n_layers') else None,
                'hidden_nf': egnn.hidden_nf if hasattr(egnn, 'hidden_nf') else None,
                'attention': egnn.attention if hasattr(egnn, 'attention') else None,
                'normalization_factor': egnn.normalization_factor if hasattr(egnn, 'normalization_factor') else None,
                'aggregation_method': egnn.aggregation_method if hasattr(egnn, 'aggregation_method') else None,
            }
            key_params['EGNNå‚æ•°'] = egnn_params
        
        # æ‰©æ•£å‚æ•°
        if 'diffusion_params' in hparams:
            diff = hparams['diffusion_params']
            diff_params = {
                'diffusion_steps': diff.diffusion_steps if hasattr(diff, 'diffusion_steps') else None,
                'diffusion_noise_schedule': diff.diffusion_noise_schedule if hasattr(diff, 'diffusion_noise_schedule') else None,
                'diffusion_loss_type': diff.diffusion_loss_type if hasattr(diff, 'diffusion_loss_type') else None,
            }
            key_params['æ‰©æ•£å‚æ•°'] = diff_params
        
        # æ¨¡å‹æ¨¡å¼
        key_params['æ¨¡å‹æ¨¡å¼'] = hparams.get('mode')
        key_params['å£è¢‹è¡¨ç¤º'] = hparams.get('pocket_representation')
        key_params['æ•°æ®é›†'] = hparams.get('dataset')
        
        self.analysis_results['hyperparameters'] = key_params
        return key_params
    
    def count_parameters(self, state_dict):
        """ç»Ÿè®¡å‚æ•°æ•°é‡"""
        total_params = 0
        trainable_params = 0
        
        param_details = {}
        
        for name, param in state_dict.items():
            num_params = param.numel()
            total_params += num_params
            trainable_params += num_params  # ä»state_dictä¸­çš„éƒ½æ˜¯å¯è®­ç»ƒçš„
            
            param_details[name] = {
                'shape': list(param.shape),
                'num_params': num_params,
                'dtype': str(param.dtype)
            }
        
        return {
            'total': total_params,
            'trainable': trainable_params,
            'details': param_details
        }
    
    def analyze_model_architecture(self):
        """åˆ†ææ¨¡å‹æ¶æ„"""
        if 'state_dict' not in self.checkpoint:
            return {"é”™è¯¯": "æœªæ‰¾åˆ°state_dict"}
        
        state_dict = self.checkpoint['state_dict']
        
        # ç»Ÿè®¡å‚æ•°
        param_stats = self.count_parameters(state_dict)
        
        # æŒ‰æ¨¡å—åˆ†ç»„å‚æ•°
        module_groups = self._group_parameters_by_module(state_dict)
        
        # åˆ†æEGNNå±‚
        egnn_layers = self._analyze_egnn_layers(state_dict)
        
        # ç»Ÿè®¡å„æ¨¡å—å‚æ•°æ•°é‡
        module_param_counts = {}
        for module_name, params in module_groups.items():
            count = sum(p['num_params'] for p in params)
            module_param_counts[module_name] = count
        
        architecture = {
            'æ€»å‚æ•°æ•°': f"{param_stats['total']:,}",
            'å¯è®­ç»ƒå‚æ•°æ•°': f"{param_stats['trainable']:,}",
            'å‚æ•°å¤§å°(MB)': f"{param_stats['total'] * 4 / (1024**2):.2f}",  # å‡è®¾float32
            'æ¨¡å—å‚æ•°åˆ†å¸ƒ': module_param_counts,
            'EGNNå±‚åˆ†æ': egnn_layers,
            'å‚æ•°è¯¦æƒ…': param_stats['details']
        }
        
        self.analysis_results['architecture'] = architecture
        return architecture
    
    def _group_parameters_by_module(self, state_dict):
        """æŒ‰æ¨¡å—åˆ†ç»„å‚æ•°"""
        groups = {}
        
        for name, param in state_dict.items():
            # ç§»é™¤å‰ç¼€ï¼ˆå¦‚æœæœ‰ï¼‰
            clean_name = name.replace('model.', '').replace('_dynamics.', '')
            
            # æå–æ¨¡å—åï¼ˆç¬¬ä¸€ä¸ªç‚¹ä¹‹å‰çš„éƒ¨åˆ†ï¼‰
            parts = clean_name.split('.')
            if len(parts) > 0:
                module_name = parts[0]
                if module_name not in groups:
                    groups[module_name] = []
                
                groups[module_name].append({
                    'name': clean_name,
                    'shape': list(param.shape),
                    'num_params': param.numel()
                })
        
        return groups
    
    def _analyze_egnn_layers(self, state_dict):
        """åˆ†æEGNNå±‚ç»“æ„"""
        egnn_info = {
            'num_layers': 0,
            'layers': []
        }
        
        # æŸ¥æ‰¾EGNNå±‚ (ä¿®å¤: ä½¿ç”¨ e_block_ è€Œä¸æ˜¯ e_blocks.)
        layer_nums = set()
        for name in state_dict.keys():
            if 'egnn' in name and '.e_block_' in name:
                # æå–å±‚å·
                try:
                    # æ ¼å¼: ddpm.dynamics.egnn.e_block_0.gcl_0...
                    parts = name.split('.e_block_')
                    if len(parts) > 1:
                        layer_num = int(parts[1].split('.')[0])
                        layer_nums.add(layer_num)
                except:
                    continue
        
        egnn_info['num_layers'] = len(layer_nums)
        
        # åˆ†ææ¯ä¸€å±‚
        for layer_num in sorted(layer_nums):
            layer_params = {}
            layer_param_count = 0
            
            for name, param in state_dict.items():
                # ä¿®å¤: ä½¿ç”¨ e_block_ è€Œä¸æ˜¯ e_blocks.
                if f'.e_block_{layer_num}.' in name:
                    layer_params[name] = {
                        'shape': list(param.shape),
                        'num_params': param.numel()
                    }
                    layer_param_count += param.numel()
            
            egnn_info['layers'].append({
                'layer_num': layer_num,
                'num_params': layer_param_count,
                'num_submodules': len(layer_params)
            })
        
        return egnn_info
    
    def analyze_optimizer_state(self):
        """åˆ†æä¼˜åŒ–å™¨çŠ¶æ€"""
        if 'optimizer_states' not in self.checkpoint:
            return {"é”™è¯¯": "æœªæ‰¾åˆ°ä¼˜åŒ–å™¨çŠ¶æ€"}
        
        opt_states = self.checkpoint['optimizer_states']
        
        if len(opt_states) == 0:
            return {"é”™è¯¯": "ä¼˜åŒ–å™¨çŠ¶æ€ä¸ºç©º"}
        
        # é€šå¸¸åªæœ‰ä¸€ä¸ªä¼˜åŒ–å™¨
        opt_state = opt_states[0]
        
        info = {
            'çŠ¶æ€é”®': list(opt_state.keys()),
            'å‚æ•°ç»„æ•°é‡': len(opt_state.get('param_groups', [])),
        }
        
        # è·å–å­¦ä¹ ç‡ç­‰ä¿¡æ¯
        if 'param_groups' in opt_state:
            param_groups = opt_state['param_groups']
            if len(param_groups) > 0:
                pg = param_groups[0]
                info['å­¦ä¹ ç‡'] = pg.get('lr')
                info['ä¼˜åŒ–å™¨ç±»å‹'] = str(type(pg)).split('.')[-1].replace("'>", "")
                info['betas'] = pg.get('betas')
                info['eps'] = pg.get('eps')
                info['weight_decay'] = pg.get('weight_decay')
        
        # ç»Ÿè®¡çŠ¶æ€ä¿¡æ¯
        if 'state' in opt_state:
            state = opt_state['state']
            info['çŠ¶æ€å‚æ•°æ•°é‡'] = len(state)
            
            # é‡‡æ ·ç¬¬ä¸€ä¸ªå‚æ•°çš„çŠ¶æ€ä¿¡æ¯
            if len(state) > 0:
                first_key = list(state.keys())[0]
                first_state = state[first_key]
                info['çŠ¶æ€åŒ…å«çš„é”®'] = list(first_state.keys())
        
        self.analysis_results['optimizer'] = info
        return info
    
    def analyze_lr_scheduler(self):
        """åˆ†æå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        if 'lr_schedulers' not in self.checkpoint:
            return {"ä¿¡æ¯": "æœªä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨"}
        
        lr_schedulers = self.checkpoint['lr_schedulers']
        
        if len(lr_schedulers) == 0:
            return {"ä¿¡æ¯": "å­¦ä¹ ç‡è°ƒåº¦å™¨åˆ—è¡¨ä¸ºç©º"}
        
        scheduler = lr_schedulers[0]
        
        info = {
            'è°ƒåº¦å™¨é”®': list(scheduler.keys()),
            'æœ€åepoch': scheduler.get('last_epoch'),
            '_step_count': scheduler.get('_step_count'),
        }
        
        self.analysis_results['lr_scheduler'] = info
        return info
    
    def analyze_callbacks(self):
        """åˆ†æå›è°ƒå‡½æ•°çŠ¶æ€"""
        if 'callbacks' not in self.checkpoint:
            return {"ä¿¡æ¯": "æœªæ‰¾åˆ°å›è°ƒå‡½æ•°çŠ¶æ€"}
        
        callbacks = self.checkpoint['callbacks']
        
        info = {
            'å›è°ƒå‡½æ•°': list(callbacks.keys())
        }
        
        # åˆ†æModelCheckpointå›è°ƒ
        for key in callbacks.keys():
            if 'ModelCheckpoint' in key:
                mc = callbacks[key]
                info['æœ€ä½³æ¨¡å‹å¾—åˆ†'] = mc.get('best_model_score')
                info['æœ€ä½³æ¨¡å‹è·¯å¾„'] = mc.get('best_model_path')
        
        self.analysis_results['callbacks'] = info
        return info
    
    def generate_statistics(self):
        """ç”Ÿæˆç»Ÿè®¡æ‘˜è¦"""
        stats = {}
        
        # ä»architectureä¸­æå–
        if 'architecture' in self.analysis_results:
            arch = self.analysis_results['architecture']
            stats['æ€»å‚æ•°æ•°'] = arch.get('æ€»å‚æ•°æ•°')
            stats['å‚æ•°å¤§å°'] = arch.get('å‚æ•°å¤§å°(MB)')
            
            # EGNNå±‚æ•°
            if 'EGNNå±‚åˆ†æ' in arch:
                egnn = arch['EGNNå±‚åˆ†æ']
                stats['EGNNå±‚æ•°'] = egnn.get('num_layers')
        
        # ä»hyperparametersä¸­æå–
        if 'hyperparameters' in self.analysis_results:
            hp = self.analysis_results['hyperparameters']
            if 'EGNNå‚æ•°' in hp:
                egnn_params = hp['EGNNå‚æ•°']
                stats['éšè—å±‚ç»´åº¦'] = egnn_params.get('hidden_nf')
                stats['æ³¨æ„åŠ›æœºåˆ¶'] = egnn_params.get('attention')
            
            if 'æ‰©æ•£å‚æ•°' in hp:
                diff_params = hp['æ‰©æ•£å‚æ•°']
                stats['æ‰©æ•£æ­¥æ•°'] = diff_params.get('diffusion_steps')
        
        # è®­ç»ƒè¿›åº¦
        if 'structure' in self.analysis_results:
            struct = self.analysis_results['structure']
            stats['è®­ç»ƒè½®æ•°'] = struct.get('è®­ç»ƒè½®æ•°')
            stats['å…¨å±€æ­¥æ•°'] = struct.get('å…¨å±€æ­¥æ•°')
        
        self.analysis_results['statistics'] = stats
        return stats
    
    def analyze_all(self):
        """æ‰§è¡Œæ‰€æœ‰åˆ†æ"""
        print("\n[æ­¥éª¤ 1/8] åˆ†ææ–‡ä»¶ä¿¡æ¯...")
        self.get_file_info()
        
        print("[æ­¥éª¤ 2/8] åˆ†ææ£€æŸ¥ç‚¹ç»“æ„...")
        self.analyze_checkpoint_structure()
        
        print("[æ­¥éª¤ 3/8] åˆ†æè¶…å‚æ•°...")
        self.analyze_hyperparameters()
        
        print("[æ­¥éª¤ 4/8] åˆ†ææ¨¡å‹æ¶æ„...")
        self.analyze_model_architecture()
        
        print("[æ­¥éª¤ 5/8] åˆ†æä¼˜åŒ–å™¨çŠ¶æ€...")
        self.analyze_optimizer_state()
        
        print("[æ­¥éª¤ 6/8] åˆ†æå­¦ä¹ ç‡è°ƒåº¦å™¨...")
        self.analyze_lr_scheduler()
        
        print("[æ­¥éª¤ 7/8] åˆ†æå›è°ƒå‡½æ•°...")
        self.analyze_callbacks()
        
        print("[æ­¥éª¤ 8/8] ç”Ÿæˆç»Ÿè®¡æ‘˜è¦...")
        self.generate_statistics()
        
        print("\n[å®Œæˆ] æ‰€æœ‰åˆ†æå®Œæˆ\n")
        
        return self.analysis_results
    
    def save_json_report(self, output_path):
        """ä¿å­˜JSONæ ¼å¼çš„åˆ†ææŠ¥å‘Š"""
        # è½¬æ¢ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
        def convert_to_serializable(obj):
            if isinstance(obj, torch.Tensor):
                return obj.detach().cpu().numpy().tolist()
            elif isinstance(obj, (np.integer, np.floating)):
                return obj.item()
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, torch.dtype):
                return str(obj)
            elif isinstance(obj, dict):
                return {k: convert_to_serializable(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_to_serializable(item) for item in obj]
            else:
                return obj
        
        serializable_results = convert_to_serializable(self.analysis_results)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        print(f"[ä¿å­˜] JSONæŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
    
    def generate_markdown_report(self, output_path):
        """ç”ŸæˆMarkdownæ ¼å¼çš„è¯¦ç»†åˆ†ææŠ¥å‘Š"""
        md_lines = []
        
        # æ ‡é¢˜
        md_lines.append("# DiffSBDD æ¨¡å‹æ£€æŸ¥ç‚¹åˆ†ææŠ¥å‘Š")
        md_lines.append("")
        md_lines.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        md_lines.append("")
        md_lines.append("---")
        md_lines.append("")
        
        # 1. æ‰§è¡Œæ‘˜è¦
        md_lines.append("## ğŸ“Š æ‰§è¡Œæ‘˜è¦")
        md_lines.append("")
        if 'statistics' in self.analysis_results:
            stats = self.analysis_results['statistics']
            md_lines.append("| æŒ‡æ ‡ | å€¼ |")
            md_lines.append("|------|------|")
            for key, value in stats.items():
                md_lines.append(f"| {key} | {value} |")
        md_lines.append("")
        md_lines.append("---")
        md_lines.append("")
        
        # 2. æ–‡ä»¶ä¿¡æ¯
        md_lines.append("## ğŸ“ æ–‡ä»¶ä¿¡æ¯")
        md_lines.append("")
        if 'file_info' in self.analysis_results:
            info = self.analysis_results['file_info']
            for key, value in info.items():
                md_lines.append(f"- **{key}**: `{value}`")
        md_lines.append("")
        md_lines.append("---")
        md_lines.append("")
        
        # 3. æ£€æŸ¥ç‚¹ç»“æ„
        md_lines.append("## ğŸ—ï¸ æ£€æŸ¥ç‚¹ç»“æ„")
        md_lines.append("")
        if 'structure' in self.analysis_results:
            struct = self.analysis_results['structure']
            
            md_lines.append("### é¡¶å±‚é”®")
            md_lines.append("```")
            for key in struct.get('é¡¶å±‚é”®', []):
                md_lines.append(f"  - {key}")
            md_lines.append("```")
            md_lines.append("")
            
            if 'è®­ç»ƒè½®æ•°' in struct:
                md_lines.append(f"- **è®­ç»ƒè½®æ•°**: {struct['è®­ç»ƒè½®æ•°']}")
            if 'å…¨å±€æ­¥æ•°' in struct:
                md_lines.append(f"- **å…¨å±€æ­¥æ•°**: {struct['å…¨å±€æ­¥æ•°']}")
        md_lines.append("")
        md_lines.append("---")
        md_lines.append("")
        
        # 4. è¶…å‚æ•°é…ç½®
        md_lines.append("## âš™ï¸ è¶…å‚æ•°é…ç½®")
        md_lines.append("")
        if 'hyperparameters' in self.analysis_results:
            hp = self.analysis_results['hyperparameters']
            
            for category, params in hp.items():
                if isinstance(params, dict):
                    md_lines.append(f"### {category}")
                    md_lines.append("")
                    md_lines.append("| å‚æ•° | å€¼ |")
                    md_lines.append("|------|------|")
                    for key, value in params.items():
                        md_lines.append(f"| {key} | {value} |")
                    md_lines.append("")
                else:
                    md_lines.append(f"- **{category}**: {params}")
                    md_lines.append("")
        md_lines.append("---")
        md_lines.append("")
        
        # 5. æ¨¡å‹æ¶æ„
        md_lines.append("## ğŸ§  æ¨¡å‹æ¶æ„åˆ†æ")
        md_lines.append("")
        if 'architecture' in self.analysis_results:
            arch = self.analysis_results['architecture']
            
            # æ€»è§ˆ
            md_lines.append("### æ¶æ„æ€»è§ˆ")
            md_lines.append("")
            md_lines.append("| æŒ‡æ ‡ | å€¼ |")
            md_lines.append("|------|------|")
            md_lines.append(f"| æ€»å‚æ•°æ•° | {arch.get('æ€»å‚æ•°æ•°', 'N/A')} |")
            md_lines.append(f"| å¯è®­ç»ƒå‚æ•°æ•° | {arch.get('å¯è®­ç»ƒå‚æ•°æ•°', 'N/A')} |")
            md_lines.append(f"| å‚æ•°å¤§å° | {arch.get('å‚æ•°å¤§å°(MB)', 'N/A')} MB |")
            md_lines.append("")
            
            # æ¨¡å—å‚æ•°åˆ†å¸ƒ
            if 'æ¨¡å—å‚æ•°åˆ†å¸ƒ' in arch:
                md_lines.append("### æ¨¡å—å‚æ•°åˆ†å¸ƒ")
                md_lines.append("")
                md_lines.append("| æ¨¡å— | å‚æ•°æ•°é‡ | å æ¯” |")
                md_lines.append("|------|----------|------|")
                
                total = sum(arch['æ¨¡å—å‚æ•°åˆ†å¸ƒ'].values())
                for module, count in sorted(
                    arch['æ¨¡å—å‚æ•°åˆ†å¸ƒ'].items(),
                    key=lambda x: x[1],
                    reverse=True
                ):
                    percentage = (count / total * 100) if total > 0 else 0
                    md_lines.append(f"| {module} | {count:,} | {percentage:.2f}% |")
                md_lines.append("")
            
            # EGNNå±‚åˆ†æ
            if 'EGNNå±‚åˆ†æ' in arch:
                egnn = arch['EGNNå±‚åˆ†æ']
                md_lines.append("### EGNN å±‚ç»“æ„")
                md_lines.append("")
                md_lines.append(f"**æ€»å±‚æ•°**: {egnn.get('num_layers', 0)}")
                md_lines.append("")
                
                if 'layers' in egnn and len(egnn['layers']) > 0:
                    md_lines.append("| å±‚ç¼–å· | å‚æ•°æ•°é‡ | å­æ¨¡å—æ•° |")
                    md_lines.append("|--------|----------|----------|")
                    for layer in egnn['layers']:
                        md_lines.append(
                            f"| Layer {layer['layer_num']} | "
                            f"{layer['num_params']:,} | "
                            f"{layer['num_submodules']} |"
                        )
                    md_lines.append("")
        md_lines.append("---")
        md_lines.append("")
        
        # 6. ä¼˜åŒ–å™¨çŠ¶æ€
        md_lines.append("## ğŸ¯ ä¼˜åŒ–å™¨çŠ¶æ€")
        md_lines.append("")
        if 'optimizer' in self.analysis_results:
            opt = self.analysis_results['optimizer']
            md_lines.append("| å‚æ•° | å€¼ |")
            md_lines.append("|------|------|")
            for key, value in opt.items():
                if key not in ['çŠ¶æ€é”®', 'çŠ¶æ€åŒ…å«çš„é”®']:
                    md_lines.append(f"| {key} | {value} |")
            md_lines.append("")
        md_lines.append("---")
        md_lines.append("")
        
        # 7. å­¦ä¹ ç‡è°ƒåº¦å™¨
        md_lines.append("## ğŸ“ˆ å­¦ä¹ ç‡è°ƒåº¦å™¨")
        md_lines.append("")
        if 'lr_scheduler' in self.analysis_results:
            lr_sched = self.analysis_results['lr_scheduler']
            for key, value in lr_sched.items():
                md_lines.append(f"- **{key}**: {value}")
        md_lines.append("")
        md_lines.append("---")
        md_lines.append("")
        
        # 8. å›è°ƒå‡½æ•°
        md_lines.append("## ğŸ”” å›è°ƒå‡½æ•°")
        md_lines.append("")
        if 'callbacks' in self.analysis_results:
            cb = self.analysis_results['callbacks']
            for key, value in cb.items():
                md_lines.append(f"- **{key}**: {value}")
        md_lines.append("")
        md_lines.append("---")
        md_lines.append("")
        
        # 9. è¯¦ç»†å‚æ•°åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œå¤ªé•¿çš„è¯å¯ä»¥æ³¨é‡Šæ‰ï¼‰
        md_lines.append("## ğŸ“‹ è¯¦ç»†å‚æ•°åˆ—è¡¨")
        md_lines.append("")
        md_lines.append("<details>")
        md_lines.append("<summary>ç‚¹å‡»å±•å¼€å®Œæ•´å‚æ•°åˆ—è¡¨ï¼ˆå¯èƒ½å¾ˆé•¿ï¼‰</summary>")
        md_lines.append("")
        
        if 'architecture' in self.analysis_results:
            arch = self.analysis_results['architecture']
            if 'å‚æ•°è¯¦æƒ…' in arch:
                md_lines.append("| å‚æ•°å | å½¢çŠ¶ | å‚æ•°æ•° | æ•°æ®ç±»å‹ |")
                md_lines.append("|--------|------|--------|----------|")
                
                # åªæ˜¾ç¤ºå‰100ä¸ªå‚æ•°ï¼Œé¿å…å¤ªé•¿
                param_details = arch['å‚æ•°è¯¦æƒ…']
                for i, (name, details) in enumerate(param_details.items()):
                    if i >= 100:
                        md_lines.append(f"| ... | ... | ... | ... |")
                        md_lines.append(f"| *çœç•¥å‰©ä½™ {len(param_details) - 100} ä¸ªå‚æ•°* | | | |")
                        break
                    md_lines.append(
                        f"| {name} | "
                        f"{details['shape']} | "
                        f"{details['num_params']:,} | "
                        f"{details['dtype']} |"
                    )
        
        md_lines.append("")
        md_lines.append("</details>")
        md_lines.append("")
        md_lines.append("---")
        md_lines.append("")
        
        # 10. ç»“è®ºä¸å»ºè®®
        md_lines.append("## ğŸ’¡ ç»“è®ºä¸å»ºè®®")
        md_lines.append("")
        
        # åŸºäºåˆ†æç»“æœç»™å‡ºå»ºè®®
        if 'statistics' in self.analysis_results:
            stats = self.analysis_results['statistics']
            
            md_lines.append("### æ¨¡å‹ç‰¹å¾")
            md_lines.append("")
            
            # å‚æ•°é‡è¯„ä¼°
            total_params_str = stats.get('æ€»å‚æ•°æ•°', '0')
            total_params = int(total_params_str.replace(',', '')) if total_params_str != 'N/A' else 0
            
            if total_params < 1_000_000:
                md_lines.append(f"- âœ… **è½»é‡çº§æ¨¡å‹**: çº¦ {total_params/1000:.1f}K å‚æ•°ï¼Œé€‚åˆå¿«é€Ÿè®­ç»ƒå’Œæ¨ç†")
            elif total_params < 10_000_000:
                md_lines.append(f"- âœ… **ä¸­ç­‰è§„æ¨¡æ¨¡å‹**: çº¦ {total_params/1_000_000:.2f}M å‚æ•°ï¼Œå¹³è¡¡æ€§èƒ½ä¸æ•ˆç‡")
            else:
                md_lines.append(f"- âš ï¸ **å¤§å‹æ¨¡å‹**: çº¦ {total_params/1_000_000:.2f}M å‚æ•°ï¼Œéœ€è¦è¾ƒå¤šè®¡ç®—èµ„æº")
            
            md_lines.append("")
            
            # EGNNå±‚æ•°è¯„ä¼°
            if 'EGNNå±‚æ•°' in stats:
                num_layers = stats['EGNNå±‚æ•°']
                md_lines.append(f"- **EGNNæ·±åº¦**: {num_layers} å±‚")
                if num_layers <= 4:
                    md_lines.append("  - è¾ƒæµ…çš„ç½‘ç»œï¼Œè®­ç»ƒå¿«é€Ÿï¼Œé€‚åˆè¿­ä»£å­¦ä¹ ")
                elif num_layers <= 8:
                    md_lines.append("  - ä¸­ç­‰æ·±åº¦ï¼Œè‰¯å¥½çš„è¡¨è¾¾èƒ½åŠ›")
                else:
                    md_lines.append("  - æ·±å±‚ç½‘ç»œï¼Œå¼ºå¤§çš„è¡¨è¾¾èƒ½åŠ›ä½†è®­ç»ƒè¾ƒæ…¢")
            
            md_lines.append("")
        
        md_lines.append("### è¿­ä»£å­¦ä¹ å»ºè®®")
        md_lines.append("")
        md_lines.append("åŸºäºæ­¤æ£€æŸ¥ç‚¹è¿›è¡Œè¿­ä»£å­¦ä¹ æ—¶çš„å»ºè®®ï¼š")
        md_lines.append("")
        md_lines.append("1. **å†»ç»“ç­–ç•¥**: å»ºè®®å†»ç»“å‰ 2-4 å±‚ï¼Œè®­ç»ƒå 2 å±‚")
        md_lines.append("   - ä¿ç•™åº•å±‚é€šç”¨åŒ–å­¦çŸ¥è¯†")
        md_lines.append("   - é€‚åº”ç‰¹å®šè›‹ç™½çš„ç»“åˆæ¨¡å¼")
        md_lines.append("")
        md_lines.append("2. **å­¦ä¹ ç‡è®¾ç½®**: å»ºè®®ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ (1e-4 åˆ° 1e-5)")
        md_lines.append("   - é¿å…ç ´åé¢„è®­ç»ƒæƒé‡")
        md_lines.append("   - å®ç°ç¨³å®šçš„å¾®è°ƒ")
        md_lines.append("")
        md_lines.append("3. **æ‰¹æ¬¡å¤§å°**: æ ¹æ®GPUæ˜¾å­˜è°ƒæ•´")
        md_lines.append("   - 8GB GPU: batch_size = 4-8")
        md_lines.append("   - 12GB GPU: batch_size = 8-16")
        md_lines.append("   - 24GB GPU: batch_size = 16-32")
        md_lines.append("")
        
        # ç»“å°¾
        md_lines.append("---")
        md_lines.append("")
        md_lines.append("**æŠ¥å‘Šç”Ÿæˆå·¥å…·**: `analyze_checkpoint.py`")
        md_lines.append("")
        md_lines.append(f"**æ£€æŸ¥ç‚¹**: `{self.checkpoint_path.name}`")
        md_lines.append("")
        
        # å†™å…¥æ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(md_lines))
        
        print(f"[ä¿å­˜] MarkdownæŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='DiffSBDDæ£€æŸ¥ç‚¹å…¨é¢åˆ†æå·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python analyze_checkpoint.py crossdocked_fullatom_cond.ckpt
  python analyze_checkpoint.py crossdocked_fullatom_cond.ckpt -o my_analysis
        """
    )
    
    parser.add_argument(
        'checkpoint',
        type=str,
        help='æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„'
    )
    
    parser.add_argument(
        '-o', '--output',
        type=str,
        default=None,
        help='è¾“å‡ºæ–‡ä»¶åå‰ç¼€ï¼ˆé»˜è®¤ä½¿ç”¨æ£€æŸ¥ç‚¹æ–‡ä»¶åï¼‰'
    )
    
    args = parser.parse_args()
    
    # ç¡®å®šè¾“å‡ºæ–‡ä»¶å
    if args.output is None:
        checkpoint_name = Path(args.checkpoint).stem
        output_prefix = f"{checkpoint_name}_analysis"
    else:
        output_prefix = args.output
    
    # åˆ›å»ºåˆ†æå™¨
    try:
        analyzer = CheckpointAnalyzer(args.checkpoint)
        
        # æ‰§è¡Œæ‰€æœ‰åˆ†æ
        results = analyzer.analyze_all()
        
        # ä¿å­˜æŠ¥å‘Š
        json_path = f"{output_prefix}.json"
        md_path = f"{output_prefix}.md"
        
        analyzer.save_json_report(json_path)
        analyzer.generate_markdown_report(md_path)
        
        print("\n" + "="*60)
        print("[OK] åˆ†æå®Œæˆï¼")
        print("="*60)
        print(f"\nç”Ÿæˆçš„æ–‡ä»¶:")
        print(f"  [JSON] JSONæŠ¥å‘Š: {json_path}")
        print(f"  [MD] MarkdownæŠ¥å‘Š: {md_path}")
        print(f"\nè¯·æŸ¥çœ‹MarkdownæŠ¥å‘Šä»¥è·å–è¯¦ç»†åˆ†æç»“æœã€‚\n")
        
    except Exception as e:
        print(f"\n[é”™è¯¯] {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == '__main__':
    exit(main())

