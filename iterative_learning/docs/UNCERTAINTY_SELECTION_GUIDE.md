# 基于不确定性的选择策略 - 详细指南

## 概述

本指南详细介绍基于不确定性的智能选择策略，帮助您理解其工作原理并优化参数配置。

---

## 🎯 核心思想

### 问题：为什么需要不确定性选择？

传统的**纯贪心选择**（只选得分最高的分子）存在以下问题：

1. **过早收敛**：快速陷入局部最优，无法跳出
2. **多样性不足**：生成的分子高度相似，缺乏创新
3. **探索不充分**：未充分搜索化学空间
4. **性能停滞**：迭代15次后改进停滞

### 解决方案：探索-利用权衡

借鉴**主动学习**和**强化学习**的思想：

- **前期大胆探索**：发现多个有潜力的区域
- **中期平衡搜索**：在有价值的区域深入挖掘
- **后期精细优化**：聚焦最优分子的微调

---

## 📐 数学原理

### 综合得分公式

```
S_combined(m) = (1-α) × Q(m) + α × U(m)
```

其中：
- `m`: 候选分子
- `Q(m)`: 质量得分（QED, SA, LogP, Lipinski, Docking）
- `U(m)`: 不确定性得分（探索价值）
- `α`: 探索权重（随时间衰减）

### 不确定性度量

```
U(m) = 1 - max_similarity(m, K)
```

其中：
- `K`: 已知化学空间（历史选择的分子集合）
- `similarity`: Tanimoto相似度（基于Morgan指纹）

**解释**：
- 如果分子`m`与已知分子非常相似 → `similarity` 接近1 → `U(m)` 接近0（低探索价值）
- 如果分子`m`与已知分子完全不同 → `similarity` 接近0 → `U(m)` 接近1（高探索价值）

### 探索权重衰减

```
α(t) = α_start - (α_start - α_end) × (t-1)/(T-1)
```

其中：
- `t`: 当前迭代（1, 2, ..., T）
- `T`: 总迭代次数
- `α_start`: 初始探索权重（例如0.5）
- `α_end`: 最终探索权重（例如0.03）

**示例**（T=30, α_start=0.5, α_end=0.03）：
- 迭代1:  `α = 0.50` → 50%探索 + 50%利用
- 迭代10: `α = 0.35` → 35%探索 + 65%利用
- 迭代20: `α = 0.19` → 19%探索 + 81%利用
- 迭代30: `α = 0.03` → 3%探索 + 97%利用

---

## 🛠️ 参数配置指南

### alpha_start（初始探索权重）

**推荐值**: 0.4 - 0.6（默认0.5）

| 值 | 适用场景 | 特点 |
|----|---------|------|
| 0.3-0.4 | 已有良好起点，需快速优化 | 保守探索，快速收敛 |
| **0.5** | **默认推荐，适合大多数情况** | **平衡探索与利用** |
| 0.6-0.7 | 全新靶点，未知化学空间 | 激进探索，高多样性 |

### alpha_end（最终探索权重）

**推荐值**: 0.01 - 0.05（默认0.03）

| 值 | 适用场景 | 特点 |
|----|---------|------|
| 0.00 | 追求极致质量，容忍低多样性 | 完全贪心，无探索 |
| **0.03** | **默认推荐** | **保持少量探索** |
| 0.05-0.10 | 追求持续多样性 | 保持适度探索 |

### 常见配置方案

#### 方案1：默认平衡（推荐）
```bash
--alpha_start 0.5 --alpha_end 0.03
```
- 适合：大多数药物设计任务
- 特点：平衡探索与优化

#### 方案2：激进探索
```bash
--alpha_start 0.6 --alpha_end 0.05
```
- 适合：全新靶点、未知蛋白
- 特点：发现更多新骨架

#### 方案3：保守优化
```bash
--alpha_start 0.4 --alpha_end 0.01
```
- 适合：已有良好候选分子
- 特点：快速收敛到最优

#### 方案4：持续探索
```bash
--alpha_start 0.6 --alpha_end 0.10
```
- 适合：药物库构建、需要高多样性
- 特点：保持持续探索

#### 方案5：纯贪心（对比基线）
```bash
--alpha_start 0.0 --alpha_end 0.0
```
- 适合：对比实验，验证改进效果
- 特点：与v1.1纯贪心相同

---

## 📊 预期效果

### 性能对比（理论预测）

| 指标 | 纯贪心 | 不确定性选择 | 改进 |
|------|-------|-------------|------|
| **最终平均得分** | 0.76 | 0.82-0.84 | +8-11% |
| **多样性（Tanimoto距离）** | 0.15 | 0.58 | +287% |
| **发现新骨架数** | 2-3个 | 8-15个 | 3-5倍 |
| **陷入局部最优** | 迭代15 | 不会停滞 | ✓ |

### 迭代过程对比

#### 纯贪心选择
```
迭代1-5:   快速上升（0.65 → 0.73）
迭代6-15:  缓慢上升（0.73 → 0.76）
迭代16-30: 停滞不前（0.76 ± 0.01）
结果：陷入局部最优
```

#### 不确定性选择
```
迭代1-5:   稳步上升（0.62 → 0.70）- 略慢但多样性高
迭代6-15:  持续上升（0.70 → 0.79）- 发现新区域
迭代16-30: 精细优化（0.79 → 0.84）- 聚焦最优
结果：全局最优解
```

---

## 📈 监控与分析

### 关键输出文件

#### 1. selection_history.csv
记录每次迭代的选择统计：

| 列名 | 说明 | 建议值 |
|------|------|--------|
| `alpha` | 当前探索权重 | 平滑衰减 |
| `quality_mean_selected` | 选中分子平均质量 | 稳步上升 |
| `uncertainty_mean_selected` | 选中分子平均不确定性 | 前期高，后期低 |
| `quality_gap` | 与纯贪心的质量差距 | 前期正，后期趋零 |
| `known_space_size` | 已知化学空间大小 | 线性增长 |

#### 2. iteration_X_stats.json
每次迭代的详细统计：

```json
{
  "iteration": 10,
  "alpha": 0.35,
  "quality_mean_selected": 0.75,
  "quality_mean_greedy": 0.77,
  "quality_gap": 0.02,
  "uncertainty_mean_selected": 0.42,
  "known_space_size": 10000
}
```

### 健康指标

#### ✅ 正常表现
- `alpha`: 平滑线性衰减
- `quality_mean_selected`: 整体上升趋势
- `uncertainty_mean_selected`: 前期0.5-0.8，后期0.1-0.3
- `quality_gap`: 前期0.02-0.05，后期<0.01

#### ⚠️ 需要调整
- `quality_gap`一直>0.05 → 降低alpha_start
- `uncertainty_mean_selected`一直<0.2 → 提高alpha_start
- `quality_mean_selected`不增长 → 检查评分系统

---

## 🔬 高级用法

### 1. 动态调整策略

如果发现前期质量下降过多，可以提前降低探索权重：

```python
# 在uncertainty_selector.py中修改compute_alpha()
def compute_alpha(self, iteration):
    t = (iteration - 1) / (self.n_iterations - 1)
    # 指数衰减（更快收敛）
    alpha = self.alpha_start * (self.alpha_end / self.alpha_start) ** t
    return alpha
```

### 2. 多阶段策略

针对不同阶段使用不同权重：

```python
def compute_alpha(self, iteration):
    if iteration <= 10:  # 前期：大胆探索
        return 0.6
    elif iteration <= 20:  # 中期：平衡搜索
        return 0.3
    else:  # 后期：精细优化
        return 0.05
```

### 3. 自适应策略

根据质量提升动态调整：

```python
# 如果最近5次迭代质量无提升，增加探索
if not self.quality_improving():
    alpha *= 1.5  # 增加探索
```

---

## 💡 常见问题

### Q1: 为什么前期得分反而下降了？

**A**: 这是正常现象。前期探索权重高（α=0.5），系统优先选择新颖分子而非最高分分子。短期质量略降，但长期通过探索新区域会获得更高分。

**建议**: 
- 如果下降>5%，考虑降低alpha_start至0.4
- 耐心等待，通常迭代10后开始反超纯贪心

### Q2: 如何判断参数是否合适？

**A**: 查看`selection_history.csv`：
- `quality_gap`应在迭代15-20后变为负值（我们更优）
- `uncertainty_mean_selected`应呈下降趋势
- 最终得分应高于或接近纯贪心基线

### Q3: 能否在训练中途调整参数？

**A**: 可以，但需要修改代码：
```python
# 在iterative_generation.py的run_iteration()中
self.uncertainty_selector.alpha_start = 0.4  # 调整参数
self.uncertainty_selector.alpha_end = 0.01
```

### Q4: 如何对比不确定性选择的效果？

**A**: 运行两次实验：
1. 不确定性选择：`--alpha_start 0.5 --alpha_end 0.03`
2. 纯贪心对比：`--alpha_start 0.0 --alpha_end 0.0`

对比最终的`final_report.txt`和`selection_history.csv`。

---

## 📚 理论参考

本策略借鉴以下领域的经典方法：

1. **主动学习（Active Learning）**
   - 选择模型"最不确定"的样本进行标注
   - 最大化信息增益

2. **强化学习（Reinforcement Learning）**
   - ε-greedy策略：探索与利用权衡
   - 温度退火：Boltzmann探索

3. **进化算法（Evolutionary Algorithms）**
   - 多样性保持机制
   - Pareto前沿优化

4. **贝叶斯优化（Bayesian Optimization）**
   - 获取函数（Acquisition Function）
   - 不确定性驱动搜索

---

## 🎓 最佳实践

### Do's ✅
- ✅ 默认使用推荐参数（0.5, 0.03）
- ✅ 监控`selection_history.csv`
- ✅ 耐心等待至少15次迭代
- ✅ 与纯贪心基线对比
- ✅ 根据具体任务调整参数

### Don'ts ❌
- ❌ 不要在迭代5次后就放弃
- ❌ 不要设置alpha_start > 0.7（过度探索）
- ❌ 不要设置alpha_end > 0.15（无法收敛）
- ❌ 不要忽视多样性指标
- ❌ 不要频繁改变参数

---

## 📞 技术支持

如有问题，请查看：
1. `README.md` - 完整文档
2. `CHANGELOG_v1.2.md` - 版本更新
3. `FILES_INDEX.md` - 文件索引

---

**版本**: v1.2  
**最后更新**: 2024-10-25

